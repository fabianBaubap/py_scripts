{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymysql\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pymysql\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "\n",
    "def extract_and_save2(block_of_ids, iteration, query, connection):\n",
    "    try:\n",
    "        # Crear un cursor\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Ejecutar el query con el offset actual y el tamaño del lote\n",
    "        query_with_offset = query.format(block_of_ids)\n",
    "        cursor.execute(query_with_offset)\n",
    "        #print(query_with_offset)\n",
    "        # Obtener los resultados\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        # Verificar si hay resultados\n",
    "        if results:\n",
    "            # Crear un DataFrame a partir de los resultados\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            df = pd.DataFrame(results, columns=columns)\n",
    "            df['body'] = df['body'].replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "            # Guardar el DataFrame en un archivo CSV\n",
    "            csv_file_path = f'extractions/dcb/abril_2024_4_{ iteration }_data.parquet'\n",
    "            #df.to_csv(csv_file_path, index=False)\n",
    "            # Convert DataFrame to PyArrow Table\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            pq.write_table(table, csv_file_path)\n",
    "            \n",
    "\n",
    "\n",
    "            print(f'Lote { iteration } guardado en {csv_file_path}')\n",
    "        else:\n",
    "            print(\"no archivo\")    \n",
    "        \n",
    "    except pymysql.Error as e:\n",
    "        print(f'Error en la ejecución del query: {e}')\n",
    "    \n",
    "    finally:\n",
    "        # Cerrar el cursor\n",
    "        cursor.close()\n",
    "\n",
    "def parallel_extraction(id_batches, query, max_connections, connection):\n",
    "    with ThreadPoolExecutor(max_workers=max_connections) as executor:\n",
    "        for batch in id_batches:\n",
    "            executor.submit(extract_and_save2, batch, query, connection)\n",
    "\n",
    "def getAlreadyProcessedLoanIds():\n",
    "    # Directorio que contiene los archivos CSV\n",
    "    directorio = '/Users/sesaicornejorodriguez/baubap/demos/extractions/dcb/'\n",
    "\n",
    "    # Lista para almacenar los DataFrames de los archivos CSV\n",
    "    ids_unicos = set()\n",
    "\n",
    "    # Enumerar todos los archivos en el directorio\n",
    "    for archivo in os.listdir(directorio):\n",
    "        if archivo.endswith('.parquet'):\n",
    "            # Construir la ruta completa del archivo\n",
    "            ruta_archivo = os.path.join(directorio, archivo)\n",
    "            \n",
    "            # Leer solo la columna 'loanId' del archivo CSV y agregar los IDs únicos al conjunto\n",
    "            df = pd.read_csv(ruta_archivo, usecols=['loanId'])\n",
    "            ids_unicos.update(df['loanId'])\n",
    "\n",
    "    ids_concatenados = ','.join(str(id) for id in ids_unicos)\n",
    "\n",
    "    return ids_concatenados\n",
    "\n",
    "\n",
    "# Configuración de la conexión a la base de datos\n",
    "db_config = {\n",
    "    'host': 'baubap-prod-aurora3-mysql8-cluster.cluster-ro-cdtaivnuur7p.us-east-2.rds.amazonaws.com',\n",
    "    'user': '',\n",
    "    'password': '',\n",
    "    'database': 'baubap',\n",
    "}\n",
    "\n",
    "# Establecer conexión a la base de datos\n",
    "connection = pymysql.connect(**db_config)\n",
    "\n",
    "try:\n",
    "\n",
    "    # Variables de configuración\n",
    "    limit = 1000\n",
    "    block_size = 5000\n",
    "    max_connections = 1\n",
    "\n",
    "    # Query de extracción (debes proporcionar tu propio query)\n",
    "    extract_query = \"\"\"WITH L AS (\n",
    "            select loanId from loans L\n",
    "            where timestamp between UNIX_TIMESTAMP('2024-04-01') and UNIX_TIMESTAMP('2024-04-05')\n",
    "            and L.loanIdx = 0\n",
    "            and status not in ('standby')\n",
    "            and loanId in ({})\n",
    "        )\n",
    "        SELECT S.smsLogId ,S.loanId, S.userId, S.body\n",
    "        FROM smsLog S\n",
    "        JOIN L ON S.loanId = L.loanId;\"\"\"\n",
    "\n",
    "    #alreadyProcessedLoanIds = getAlreadyProcessedLoanIds()\n",
    "    #print(alreadyProcessedLoanIds)\n",
    "    queryLimit = f\"\"\"select loanId from loans L\n",
    "    where timestamp between UNIX_TIMESTAMP('2024-04-01') and UNIX_TIMESTAMP('2024-04-05')\n",
    "    and L.loanIdx = 0\n",
    "    and status not in ('standby')\"\"\"\n",
    "    #if alreadyProcessedLoanIds:\n",
    "     #   queryLimit += f\"\"\" and loanId not in ({alreadyProcessedLoanIds}); \"\"\"\n",
    "\n",
    "    print(queryLimit)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(queryLimit)\n",
    "    results = cursor.fetchall()\n",
    "   # columns = [desc[0] for desc in cursor.description]\n",
    "    all_ids_list_df = pd.DataFrame(results)\n",
    "    for i in range(0, len(all_ids_list_df), block_size):\n",
    "        print(i)\n",
    "        block_of_ids = all_ids_list_df[i:i + block_size]\n",
    "\n",
    "        id_list = ','.join(map(str, block_of_ids[0].tolist()))\n",
    "        if id_list:\n",
    "            extract_and_save2(id_list, i, extract_query, connection)\n",
    "        \n",
    "    cursor.close()\n",
    "\n",
    "finally:\n",
    "    # Cerrar la conexión a la base de datos al finalizar\n",
    "    connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     ids_concatenados \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ids_unicos)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ids_concatenados \n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgetAlreadyProcessedLoanIds2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mgetAlreadyProcessedLoanIds2\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m ids_unicos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Enumerar todos los archivos en el directorio\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m archivo \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(directorio):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m archivo\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Construir la ruta completa del archivo\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(archivo)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def getAlreadyProcessedLoanIds2():\n",
    "    # Directorio que contiene los archivos CSV\n",
    "    directorio = '/Users/sesaicornejorodriguez/baubap/demos/extractions/dcb/'\n",
    "\n",
    "    # Lista para almacenar los DataFrames de los archivos CSV\n",
    "    ids_unicos = set()\n",
    "\n",
    "    # Enumerar todos los archivos en el directorio\n",
    "    for archivo in os.listdir(directorio):\n",
    "        if archivo.endswith('.csv'):\n",
    "            # Construir la ruta completa del archivo\n",
    "            print(archivo)\n",
    "            ruta_archivo = os.path.join(directorio, archivo)\n",
    "            \n",
    "            # Leer solo la columna 'loanId' del archivo CSV y agregar los IDs únicos al conjunto\n",
    "            df = pd.read_parquet(ruta_archivo, usecols=['loanId'])\n",
    "            ids_unicos.update(df['loanId'])\n",
    "\n",
    "    ids_concatenados = ','.join(str(id) for id in ids_unicos)\n",
    "\n",
    "    return ids_concatenados \n",
    "\n",
    "print(getAlreadyProcessedLoanIds2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al leer el archivo CSV: Error tokenizing data. C error: Expected 4 fields in line 3, saw 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directorio = '/Users/sesaicornejorodriguez/baubap/demos/extractions/dcb/febrero_2024_2_30000_data.csv'\n",
    "import csv\n",
    "\n",
    "try:\n",
    "\n",
    "    file = pd.read_csv(directorio, sep=',', encoding=\"utf-8\" , quoting=csv.QUOTE_NONE )\n",
    "\n",
    "except pd.errors.ParserError as e:\n",
    "    print(\"Error al leer el archivo CSV:\", e)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
